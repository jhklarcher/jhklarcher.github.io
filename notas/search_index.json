[
["index.html", "Notas Início", " Notas Início Notas de aula de estatística "],
["regressão-linear-múltipla.html", "1 Regressão Linear Múltipla 1.1 Equações 1.2 Estimador para a variância do Erro 1.3 Testes de Hipótese para a Regressão 1.4 Premissas do Modelo 1.5 Gerando Previsões 1.6 Pontos Influêntes no Modelo", " 1 Regressão Linear Múltipla Baseado em Montgomery and Runger (2003). 1.1 Equações A equação geral para o modelo de regressão linear múltipla é: \\[ y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_k x_{ik} + \\epsilon_i \\quad\\quad\\quad\\quad i = 1, 2, ..., n\\] Onde os valores de \\(\\beta_k\\) são os coeficientes da regressão para cada variável e \\(\\beta_0\\) é o coeficiente de Intersecção no eixo da variável de interesse. \\(k\\) é o numero de variáveis de regressão e \\(n\\) é o número de observações. \\(x_{ik}\\) é a i-ésima observação da k-ésima variável de regressão. \\(\\epsilon_i\\) é o erro do modelo para a observação i. Em forma matricial, pode-se escrever: \\[ \\boldsymbol{y=X\\beta +\\epsilon} \\] Onde: \\[ \\boldsymbol{y} = \\begin{bmatrix} y_{ 1 } \\\\ y_{ 2 } \\\\ \\vdots \\\\ y_{ n } \\end{bmatrix} \\quad \\boldsymbol{X}=\\begin{bmatrix} 1 &amp; x_{ 11 } &amp; x_{ 12 } &amp; \\cdots &amp; x_{ 1k } \\\\ 1 &amp; x_{ 11 } &amp; x_{ 11 } &amp; \\cdots &amp; x_{ 2k } \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{ 11 } &amp; x_{ 11 } &amp; \\cdots &amp; x_{ nk } \\end{bmatrix} \\quad \\boldsymbol{ \\beta }=\\begin{bmatrix} \\beta _{ 0 } \\\\ \\beta _{ 1 } \\\\ \\vdots \\\\ \\beta _{ k } \\end{bmatrix} \\quad \\boldsymbol{ \\epsilon }=\\begin{bmatrix} \\epsilon _{ 1 } \\\\ \\epsilon _{ 2 } \\\\ \\vdots \\\\ \\epsilon _{ n } \\end{bmatrix} \\] \\(n\\): número de linhas da matriz X \\(p\\): número de colunas da matriz X \\(k=p-1\\) Minimizando o vetor de erro com o método dos mínimos quadrados, encontra-se o melhor estimador d para os coeficientes: o vetor \\(\\hat{\\beta}\\). \\(\\hat{\\beta}\\) pode ser escrito como: \\[ \\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X}&#39;\\boldsymbol{X})^{-1}\\boldsymbol{X}&#39;\\boldsymbol{y} \\] O modelo ajustado será: \\[ \\boldsymbol{\\hat{y}}=\\boldsymbol{X \\hat{\\beta}} \\] A diferença entre o valor ajustado e o valor da observação é chamado de resíduo e pode ser descrito como: \\[\\boldsymbol{e} = \\boldsymbol{y}-\\boldsymbol{\\hat{y}} \\] 1.2 Estimador para a variância do Erro \\[ \\hat{\\sigma}^2 = \\frac{ \\sum_{ i=1}^{n}e_{i}^{2} }{ n-p } = \\frac{ SS_E }{n-p} \\] \\(SS_E\\): soma dos quadrados dos resíduos. \\((n-p)\\): graus de liberdade dos resíduos. 1.3 Testes de Hipótese para a Regressão 1.3.1 Teste para a significância da regressão (ANOVA) \\(H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\) \\(H_1: \\beta_j \\ne 0\\), para pelo menos um valor de j. As estatística do teste é: \\[ F_0 = \\frac{SS_R / k}{SS_E / (n-p)} = \\frac{MS_R}{MS_E} \\] \\[ SS_E = \\boldsymbol{e}&#39; \\boldsymbol{e} = \\boldsymbol{y}&#39;\\boldsymbol{y} - \\boldsymbol{\\hat{\\beta}}&#39;\\boldsymbol{X}&#39;\\boldsymbol{y} \\] \\[ SS_R = \\boldsymbol{\\hat{\\beta}}&#39;\\boldsymbol{X}&#39;\\boldsymbol{y} - \\frac{ \\left( \\sum_{i=1}^{n} y_i \\right)^2 }{n} \\] \\[ SS_T = SS_R+SS_E = \\boldsymbol{y}&#39;\\boldsymbol{y} - \\frac{ \\left( \\sum_{i=1}^{n} y_i \\right)^2 }{n} \\] Se \\(f_0 &gt;f_{\\alpha, k, n-p}\\), \\(H_0\\) deve ser regeitada. \\(f_{\\alpha, k, n-p}\\) pode ser consultado em alguma tabela ou software. Também pode-se encontrar o p-valor para a estatística F. 1.3.2 R² e R² ajustado \\[ R^2 = \\frac{SS_R}{SS_T} = 1-\\frac{SS_E}{SS_T} \\] \\[ R^2_{aj} = 1-\\frac{SS_E/(n-p)}{SS_T/(n-1)} \\] 1.3.3 Teste para os coeficientes individualmente \\(H_0: \\beta_j=0\\) \\(H_1: \\beta_j \\ne 0\\) Estatística do teste: \\[ T_0 = \\frac{ \\hat{\\beta_j} }{\\sqrt{ \\hat{\\sigma}^2 C_{jj}}} = \\frac{ \\hat{\\beta_j} }{se(\\hat{\\beta_j})} \\] Onde \\(C = ( \\boldsymbol{X}&#39; \\boldsymbol{X} )^{-1}\\), então \\(C_{jj}\\) é o elemento da diagonal de \\(\\boldsymbol{C}\\). \\(se(\\hat{\\beta_j})\\) é o erro padrão do coeficiente de \\(\\hat{\\beta_j}\\). \\(H_0\\) é rejeitada se \\(|t_0|&gt;t_{\\alpha/2, n-p}\\). Também pode ser utilizado o p-valor para a estatística t. 1.4 Premissas do Modelo O modelo de gressão linear pelo método dos mínimos quadrados tem como premissa a normalidade dos resíduos. Essa premissa pode ser verificada por método gráfico ou por testes de normalidade, como Shapiro-Wilk e Kolmogorov-Smirnov (comparando ocm distribuição normal). 1.5 Gerando Previsões Assumindo que \\(\\boldsymbol{X_0}\\) é uma matriz com novas observações, a correspondente \\(\\boldsymbol{y_0}\\) será dada por: \\[ \\boldsymbol{y_0} = \\boldsymbol{X_0}\\boldsymbol{\\hat{\\beta}} \\] 1.6 Pontos Influêntes no Modelo Para se avaliar os pontos mais influentes no modelo se utiliza a distância de Cook. Para tal utiliza-se: \\[ \\boldsymbol{H} = \\boldsymbol{X}(\\boldsymbol{X}&#39;\\boldsymbol{X})^{-1} \\boldsymbol{X}&#39; \\] \\(h_{ii}\\) representa os elementos da diagonal de \\(H\\). O resíduo estudentizado \\(r_i\\) é: \\[ r_i = \\frac{ e_i }{ \\sqrt{ \\hat{\\sigma}^2 (1 - h_{ii}) } }, \\quad i = 1,2,...,n \\] Por fim, a distância de Cook é: \\[ D_i = \\frac{ r_i^2 }{ p } \\cdot \\frac{ h_{ii} }{ 1-h_{ii} } \\] Quando \\(D_i &gt; 1\\) oponto referente é considerado influente. Dependendo da situação essas observações podem ou não ser retiradas do modelo. Referências "],
["referências.html", "Referências", " Referências "]
]
